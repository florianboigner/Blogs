{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e28459f8-613b-4889-909e-9a77a3440fb5",
   "metadata": {},
   "source": [
    "# Blogs setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a121271d-c4b8-4297-b5f6-7f8a978d51c6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "%run CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515da3fa-4096-4cf6-9f76-cdb8cbdba01e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Config\n",
    "\n",
    "- [sempy.fabric documentation](https://learn.microsoft.com/en-us/python/api/semantic-link-sempy/sempy.fabric?view=semantic-link-python#functions)\n",
    "- [sempy_labs documentation](https://semantic-link-labs.readthedocs.io/en/stable/sempy_labs.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6b3d5d-621d-47b8-aa72-edb4d9fe874b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%pip install semantic-link-labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0e74bb-a4ef-4a6d-a8c3-a50d469ff114",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import sempy.fabric as fabric\n",
    "\n",
    "# get workspace Id\n",
    "workspaceName\n",
    "workspaceId = fabric.resolve_workspace_id(workspaceName)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277f0d56-7a05-4c7d-b833-491abb45459a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Create lakehouses\n",
    "\n",
    "- [sempy.fabric create_lakehouse](https://learn.microsoft.com/en-us/python/api/semantic-link-sempy/sempy.fabric?view=semantic-link-python#sempy-fabric-create-lakehouse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc13a7b6-1c12-4f19-a4e8-44ccf6a15ec7",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# CREATE LAKEHOUSES\n",
    "import sempy.fabric as fabric\n",
    "\n",
    "max_attempts = 10\n",
    "\n",
    "# lh_raw\n",
    "lh_raw_desc = \"Lakehouse for raw data\"\n",
    "lh1 = fabric.create_lakehouse(lh_raw, lh_raw_desc, max_attempts, workspaceName)\n",
    "\n",
    "# lh_transformed\n",
    "lh_transformed_desc = \"Lakehouse for raw data\"\n",
    "lh2 = fabric.create_lakehouse(lh_transformed, lh_transformed_desc, max_attempts, workspaceName)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab7029f-3c37-47d9-934a-cb2884b90744",
   "metadata": {},
   "source": [
    "## Create Environment\n",
    "\n",
    "Creates the default environment that will be used for all notebooks in the project. \n",
    "- [sempy_labs.create_environment](https://semantic-link-labs.readthedocs.io/en/stable/sempy_labs.html#sempy_labs.create_environment)\n",
    "\n",
    "### Manual configurations\n",
    "Following configurations need to be done __manually__ after environment creation.\n",
    "\n",
    "Adding _pypi_ libraries:\n",
    "- feedparser\n",
    "- trafilatura\n",
    "- openai\n",
    "\n",
    "Adding spark properties:\n",
    "- sprk.sql.parquet.vorder.enabled: true\n",
    "- spark.microsoft.delta.optimizeWrite.enabled: true\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89570381-0217-4025-abb3-cc9452787888",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# CREATE ENVIRONMENT\n",
    "import sempy_labs as labs\n",
    "env_description = \"Blogs custom environment\"\n",
    "\n",
    "env = labs.create_environment(env_blogs, env_description, workspaceName)\n",
    "environmentId = labs.resolve_environment_id(env_blogs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b8f16f-f409-4743-a8fb-0c6fe3b09bf7",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Import Notebooks\n",
    "\n",
    "Import notebooks using Semantic Link Labs from public Github repository: \n",
    "- [sempy_labs.import_notebook_from_web(notebook_name: str, url: str, description: str | None = None, workspace: str | None = None, overwrite: bool = False)](https://semantic-link-labs.readthedocs.io/en/stable/sempy_labs.html#sempy_labs.import_notebook_from_web)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c218a1b9-8e0c-4f70-96f5-05a8bd1568e6",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import sempy_labs as labs\n",
    "\n",
    "nb_overwrite = False\n",
    "base_url = \"https://github.com/florianboigner/Blogs/blob/main/notebooks\" # points to the folder where the notebooks are stored. Default is the public repository of the solution\n",
    "\n",
    "nb_get_feeds = \"nb_get_feeds\"\n",
    "nb_get_feeds_url = f\"{base_url}/{nb_get_feeds}.ipynb\"\n",
    "nb_get_feeds_description = \"Notebook to get feeds\"\n",
    "labs.import_notebook_from_web(nb_get_feeds, nb_get_feeds_url, nb_get_feeds_description, workspaceName, nb_overwrite)\n",
    "\n",
    "\n",
    "nb_get_blogs = \"nb_get_blogs\"\n",
    "nb_get_blogs_url = f\"{base_url}/{nb_get_blogs}.ipynb\"\n",
    "nb_get_blogs_description = \"Notebook to get blogposts\"\n",
    "labs.import_notebook_from_web(nb_get_blogs, nb_get_blogs_url, nb_get_blogs_description, workspaceName, nb_overwrite)\n",
    "\n",
    "nb_create_summary = \"nb_create_summary\"\n",
    "nb_create_summary_url = f\"{base_url}/{nb_create_summary}.ipynb\"\n",
    "nb_create_summary_description = \"Notebook to create summaries\"\n",
    "labs.import_notebook_from_web(nb_create_summary, nb_create_summary_url, nb_create_summary_description, workspaceName, nb_overwrite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2969c13a-4eba-413d-92ec-6eb4a07967ed",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Attache Lakehouse to Notebooks\n",
    "\n",
    "Thanks to Sandweep Pawar for writing [this great blog](https://fabric.guru/programmatically-removing-updating-default-lakehouse-of-a-fabric-notebook) and pointing me to the right direction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84014a59-ac0a-487c-a291-04a16df8fbe0",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import sempy.fabric as fabric\n",
    "import json\n",
    "\n",
    "def remove_all_lakehouses(notebook_name, workspaceId):\n",
    "    try:\n",
    "        nb = json.loads(notebookutils.notebook.getDefinition(notebook_name, workspaceId))\n",
    "    except:\n",
    "        print(\"Error, check notebook & workspace id\")\n",
    "\n",
    "    if 'dependencies' in nb['metadata'] and 'lakehouse' in nb['metadata']['dependencies']:\n",
    "        # Remove all lakehouses\n",
    "        nb['metadata']['dependencies']['lakehouse'] = {}\n",
    "\n",
    "    # Update the notebook definition without any lakehouses\n",
    "    notebookutils.notebook.updateDefinition(\n",
    "        name=notebook_name,\n",
    "        content=json.dumps(nb),\n",
    "        workspaceId=workspaceId\n",
    "    )\n",
    "\n",
    "    print(f\"All lakehouses have been removed from notebook '{notebook_name}'.\")\n",
    "\n",
    "def update_notebook_definition(notebook_name, lakehouse_name, workspaceId, environmentId):\n",
    "    try:\n",
    "        (notebookutils\n",
    "            .notebook\n",
    "            .updateDefinition(\n",
    "                name = notebook_name, \n",
    "                workspaceId=workspaceId,\n",
    "                defaultLakehouse=lakehouse_name, \n",
    "                defaultLakehouseWorkspace=workspaceId,\n",
    "                environmentId=environmentId,\n",
    "                environmentWorkspaceId=workspaceId\n",
    "                )\n",
    "        )\n",
    "    except:\n",
    "        print(\"Error, please check IDs\")\n",
    "    \n",
    "    print(f\"Lakehouse '{lakehouse_name}' was attached as default lakehouse to notebook '{notebook_name}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211778fd-c4ff-4e0f-8e32-0e68d52cbf50",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Variables were all defined earlier. If running only this cell make sure the variables are set\n",
    "\n",
    "# nb_get_blogs\n",
    "remove_all_lakehouses(nb_get_blogs, workspaceId)\n",
    "update_notebook_definition(nb_get_blogs, lh_raw, workspaceId, environmentId)\n",
    "\n",
    "# nb_get_feeds\n",
    "remove_all_lakehouses(nb_get_feeds, workspaceId)\n",
    "update_notebook_definition(nb_get_feeds, lh_raw, workspaceId, environmentId)\n",
    "\n",
    "# nb_create_summary\n",
    "remove_all_lakehouses(nb_create_summary, workspaceId)\n",
    "update_notebook_definition(nb_create_summary, lh_raw, workspaceId, environmentId)\n",
    "update_notebook_definition(nb_create_summary, lh_transformed, workspaceId, environmentId) # we want lh_transformed as default notebook, which is why it is attached second, overwriting the previous default"
   ]
  }
 ],
 "metadata": {
  "a365ComputeOptions": null,
  "dependencies": {
   "environment": {},
   "lakehouse": null
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "sessionKeepAliveTimeout": 0,
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
